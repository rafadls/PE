---
title: "Tarea 5"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

![](banner.png)

<center> <h1>Tarea 5: Bayesian Inference Part II</h1> </center>
<center><strong>CC6104: Statistical Thinking</strong></center>
#### **Integrantes :** 

- Rafael De La Sotta
- Felipe Ortuzar

#### **Cuerpo Docente:**

- Profesor: Felipe Bravo M.
- Auxiliar: Sebastian Bustos e Ignacio Meza D.
            
#### **Fecha límite de entrega:**

### **Índice:**

1. [Objetivo](#id1)
2. [Instrucciones](#id2)
3. [Referencias](#id3)
2. [Primera Parte: Preguntas Teóricas](#id4)
3. [Segunda Parte: Elaboración de Código](#id5)

### **Objetivo**<a name="id1"></a>

Bienvenid@s a la uuuuultima tarea del curso Statistical Thinking. Esta tarea tiene como objetivo evaluar los contenidos teóricos de la ultima parte del curso, los cuales se enfocan principalmente en aplicar inferencia bayesiana para generar regresiones lineales y estudiar métodos de obtención de la posterior mas poderosos, como es MCMC. Si aún no han visto las clases, se recomienda visitar los enlaces de las referencias.

La tarea consta de una parte teórica que busca evaluar conceptos vistos en clases. Seguido por una parte práctica con el fin de introducirlos a la programación en R enfocada en el análisis estadístico de datos. 

### **Instrucciones:**<a name="id2"></a>

- La tarea se realiza en grupos de **máximo 2 personas**. Pero no existe problema si usted desea hacerla de forma individual.
- La entrega es a través de u-cursos a más tardar el día estipulado en la misma plataforma. A las tareas atrasadas se les descontará un punto por día.
- El formato de entrega es este mismo **Rmarkdown** y un **html** con la tarea desarrollada. Por favor compruebe que todas las celdas han sido ejecutadas en el archivo html.
- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación.
- No serán revisadas tareas desarrolladas en Python.
- Está **PROHIBIDO** la copia o compartir las respuestas entre integrantes de diferentes grupos.
- Pueden realizar consultas de la tarea a través de U-cursos y/o del canal de Discord del curso. 


### **Referencias:**<a name="id3"></a>

Slides de las clases:

- [Bayesian Linear Regression](https://github.com/dccuchile/CC6104/blob/master/slides/3_3_ST-bayes_lin.pdf)
- [Markov Chain Monte Carlo](https://github.com/dccuchile/CC6104/blob/master/slides/3_4_ST-MCMC.pdf)
- [Model Evaluation and Information Criteria](https://github.com/dccuchile/CC6104/blob/master/slides/4_1_ST-eval.pdf)
- [Directed Graphical Models](https://github.com/dccuchile/CC6104/blob/master/slides/4_2_ST-dag.pdf)


Videos de las clases:

- Bayesian Linear Regression: [video 1](https://youtu.be/DrwhRshBVjM), [video 2](https://youtu.be/lgNMDCzTV9k),  [video 3](https://youtu.be/ajMucPrZDpU), [video 4](https://youtu.be/YSGWWSUMPOk), [video 5](https://youtu.be/Ma9V8Nown9Q)
- Markov Chain Monte Carlo: [video 1](https://youtu.be/gsofPiPBIeU), [video 2](https://youtu.be/EJZWaph61p4),  [video 3](https://youtu.be/jfidS22imJM), [video 4](https://youtu.be/kif9EG-sy2I), [video 5](https://youtu.be/iVgiowZvyZM), [video 6](https://youtu.be/r0BNqctisLg)
- Model Evaluation and Information Criteria: [video 1](https://youtu.be/HCCzwltLVCc), [video 2](https://youtu.be/twpZHZMmKgM),  [video 3](https://youtu.be/ny4SlO3rcTo), [video 4](https://youtu.be/6U7laePWt9M), [video 5](https://youtu.be/vE2VaK9tLV8), [video 6](https://youtu.be/wmBugs36H-4)  
- Directed Graphical Models: [video 1](https://youtu.be/2jnj-7xpK0E), [video 2](https://youtu.be/GZf8uB37noU),  [video 3](https://youtu.be/3EDdNLOrj_4), [video 4](https://youtu.be/cODS9GgepA4), [video 5](https://youtu.be/JA8H-LjAatE), [video 6](https://youtu.be/YXf0wnzvCFM)   

Documentación:

- [rethinking](https://github.com/rmcelreath/rethinking)
- [tidyr](https://tidyr.tidyverse.org)
- [purrr](https://purrr.tidyverse.org)
- [dplyr](https://dplyr.tidyverse.org)
- [ggplot2](https://ggplot2.tidyverse.org/)

# Primera Parte: Preguntas Teóricas<a name="id4"></a>
A continuación, se presentaran diferentes preguntas que abordan las temáticas vistas en clases. Por favor responda cada una de estas preguntas de forma breve, no más de 4 o 5 lineas.

#### **Pregunta 1:**

Señale algunos beneficios (no mas de dos) que nos brinda realizar una regresión lineal bayesiana sobre una regresión ajustada por mínimos cuadrados.

> 1.- La regresión Bayesiana permite entregar información previa (prior)
> 2.- Dados los resultados (posterior) es posible analizar de mejor manera el grado de incertidubre de estos.


#### **Pregunta 2:**

A continuación se presenta un modelo de regresión lineal bayesiana:

$$y_i \sim Normal(\mu, \sigma)$$
$$\mu = \beta_0 + \beta_1*x$$
$$\beta_0 \sim Normal(10,2)$$
$$\beta_1 \sim Normal(0,1)$$
$$\sigma \sim Uniform(0,50)$$

En base al modelo presentado, responda las siguientes preguntas:

- [ ] Describa que representa cada una de las lineas del modelo.

- [ ] Señale los parámetros que conforman a la regresión lineal. 

- [ ] Que objetivo cumple en el modelo lineal tener una distribución $Normal(0,1)$ en el parámetro $\beta_1$.

- [ ] Que propiedad de las regresiones lineales nos entrega $sigma$.

**Respuesta:**

**Describa que representa cada una de las lineas del modelo.**

* $y_i$ corresponde a la likelihood, la cual tiene distribución gaussiana.
* $\mu$ corresponde a una variable que representa al modelo lineal.
* $\beta_0$, $\beta_1$ y $\sigma$ corresponden a distribuciones esperadas, es decir, priors. $\beta_0$ y $\beta_1$ son gaussianas y $\sigma$ uniforme.

**Señale los parámetros que conforman a la regresión lineal**

> Los parámteros $\beta_0$ y $\beta_1$ son los interceptos de la regresión lineal. Por su parte, x corresponde a la entrada del modelo.


**Que objetivo cumple en el modelo lineal tener una distribución $Normal(0,1)$ en el parámetro $\beta_1$**

> El parámetro $\beta_1$ indica la relación entre $\mu$ y x, es por esto que se le entrega un rango de valores negativos y positivos, pasando por el cero, ya que existe la posibilidad de que la relación sea nula. 

**Que propiedad de las regresiones lineales nos entrega $sigma$.**

> $\sigma$ corresponde a la desviación estandar de la linalización, es decir, de $\mu$.

#### **Pregunta 3:**

Explique de forma resumida como funciona el algoritmo de Laplace approximation utilizado para la regresión lineal. Señale el porque existe la necesidad de aplicar este modelo y los supuestos que se realizan con este método.

**Respuesta:**

> Este método es conveniente mayormente para posteriores unimodales y simetrias, es decir, con distribuciónes aproximables por funciones gaussianas.

> Laplace aproximation asume que el posterior tiene una distribución multivariable gaussiana. Esto se puede expresar de la siguiente forma:

$$
f(θ_1,..., θ_m|d) = N( \overrightarrow{\mu,} Σ).
$$
> Donde $\overrightarrow{\mu,}$ corresponde al vector de medias y $Σ$ a la matriz de covarianza.

> Para este método no es necesario calcular el posterior normalizado, es decir, no se debe calcular f(d), lo cual es generalmente muy costoso de computar.

#### **Pregunta 4:**
Determine si las siguientes afirmaciones son verdaderas o falsas. Justifique las falsas.

- [ ] El algoritmo de metropolis hasting solo funciona si la probabilidad de saltar de B a A es la misma que de A a B.
- [ ] El algoritmo de Gibbs funciona en cualquier caso.
- [ ] El algoritmo de metropolis hasting y de Gibbs son el mismo algoritmo, pero este ultimo es una variante del primero.

> Respuesta Aquí

**[F] El algoritmo de metropolis hasting solo funciona si la probabilidad de saltar de B a A es la misma que de A a B**

> El algoritmo de Metropolis trabaja con distribuciones simetricas, donde saltar de B a A es la misma que de A a B. Por su parte, el algoritmo de metropolis hasting es más general y permite distribuciones asimetrias del posterior.

**[V] El algoritmo de Gibbs funciona en cualquier caso.**

> El algoritmo de Gibbs, al ser una variante del algoritmo de Metropolis hasting, permite distribuciones asimetrias del posterior.

**[V] El algoritmo de metropolis hasting y de Gibbs son el mismo algoritmo, pero este ultimo es una variante del primero.**

> El algoritmo de Gibbs corresponde a una variante del algoritmo de Metropolis hasting. En esta variante se recorre un parámtero a la vez, igualando la "proposal distribution" al posterior.

#### **Pregunta 5:**

El algoritmo de Gibbs es mas eficiente que el de metropolis hasting. ¿Como se logra este efecto? ¿Existe alguna limitante de esta estrategia?

> La eficiencia viene de recorrer un parámtero a la vez, igualando la "proposal distribution" al posterior en el parámetro de turno. Con esto, la "proposal distribution" siempre es aceptada.

> Este método tiene dos principales limitaciones. Primero, los casos donde no se quiete trabajar con priors conjugados. Segundo, lo casos donde se tiene un gran número de variables.


#### **Pregunta 6:**

De una ventaja y una desventaja del algoritmo HMC.

> Como desventaja se tiene un mayor costo computacional, devido al trabajo con gradientes. Como ventaja, se tiene que es má eficiente al obtener respuestas, y como consecuencia requiere de menos datos para poder obtener buenos resultados.

#### **Pregunta 7:**

Nombre y explique dos propiedades que son deseables en una cadena de Markov.

> Se desea que una cadena de Markov tenga **estacionalidad**, es decir, converger a la posterior y variar en su vecindad. Además, se desea que sea aleatoria (well mixing), es decir, que no haya correlación entre los datos, lo cual se representa como tendencias en el tiempo.

#### **Pregunta 8:**
Explique cómo cross-validation, criterios de información y regularización ayudan a mitigar o medir los problemas de underfitting y overfitting.

> Cross validation es un tipo de criterio de información y también una forma de entrenar modelos que permite la generalización
de un modelo al separar los conjuntos de entrenamiento y testeo. Gracias a esto podemos tener una métrica en particular sobre cuán intenso es el overfitting de un modelo, o también sobre el bajo ajuste de un modelo (esto al tener resultados igualmente malos en todos los batches). Por otro lado, los criterios de regularización nos permiten introducir un castigo por la complejidad de un modelo, y muchas veces los modelos más complejos son aquellos que con mayor intensidad hacen overfitting de los datos, por ejemplo, en una regresión lineal, a medida que la realizamos con más parámetros, se ajusta cada vez a los datos, perdiendo generalización. Es por esto que son necesarias funciones de pérdida que nos permitan castigar este sobreajuste.
#### **Pregunta 9:**

Diseñe una DAG para un problema causal inventado por usted de al menos 5 nodos (puede basarse en el ejemplo de Eugene Charniak) usando **Dagitty**  y considere que la DAG tenga al menos: una chain, un fork y un collider. Muestre con dagitty todas las independencias condicionales de su DAG. Explique las independencias usando las reglas de d-separación.




```{r, eval=TRUE}
#install.packages("dagitty")
library("dagitty")

my_dag <- dagitty('dag {
bb="0,0,1,1"
Caries [pos="0.644,0.745"]
Dolor [pos="0.650,0.922"]
Edad [pos="0.645,0.430"]
Frecuencia_lavado_dientes [pos="0.747,0.551"]
Ingesta_azucar [pos="0.558,0.549"]
Caries -> Dolor
Edad -> Frecuencia_lavado_dientes
Edad -> Ingesta_azucar
Frecuencia_lavado_dientes -> Caries
Ingesta_azucar -> Caries
}


')

plot(my_dag)
```



> 
Las independencias condicionales son:
- Las caries son independientes de la edad, dada la frecuencia del lavado de dientes y la ingesta de azúcar: Se observa que existe un collider de la ingesta de azucar y la frecuencia de lavado de dientes hacia caries.
- La edad es independiente del dolor, dada las caries: En consecuencia de la cadena presente entre Edad, Caries y dolor.
- La edad es independiente del dolor, dada la frecuencia del lavado de dientes: Misma razón que la anterior, pero esta vez consideramos el fork también sobre Edad.
- La frecuencia del lavado de dientes es independiente de la ingesta de azúcar, dada la edad: Este es un caso de Confounding variables, o variables de confusión en base a la edad. Esto dado que existe un fork sobre Edad.
- La ingesta de azúcar es independiente del dolor, dada las caries: Al considerar la evidencia de la caries, el camino d-conectado entre dolor e ingesta de azucar se vuelve d-separado, puesto que además no existe otro camino entre ambos nodos.

---

# Segunda Parte: Elaboración de Código<a name="id5"></a>
En la siguiente sección deberá resolver cada uno de los experimentos computacionales a través de la programación en R. Para esto se le aconseja que cree funciones en R, ya que le facilitará la ejecución de gran parte de lo solicitado.

Para el desarrollo preste mucha atención en los enunciados, ya que se le solicitará la implementación de métodos sin uso de funciones predefinidas. Por otro lado, Las librerías permitidas para desarrollar de la tarea 4 son las siguientes:

```{r, eval=TRUE}
# Manipulación de estructuras
library(tidyverse)
library(dplyr)
library(tidyr)

# Para realizar plots
library(scatterplot3d)
library(ggplot2)
library(plotly)

# Manipulación de varios plots en una imagen.
library(gridExtra)

# Análisis bayesiano
library("rethinking")
```

Si no tiene instalada la librería “rethinking”, ejecute las siguientes líneas de código para instalar la librería:

```{r, eval=FALSE}
install.packages("rethinking")
```

En caso de tener problemas al momento de instalar la librería con el código anterior, utilice las siguiente chunk:

```{r, eval=FALSE}
install.packages(c("mvtnorm","loo","coda"), repos="https://cloud.r-project.org/",dependencies=TRUE)
options(repos=c(getOption('repos'), rethinking='http://xcelab.net/R'))
install.packages('rethinking',type='source')
```


### Pregunta 1: Regresión Lineal Bayesiana

El objetivo de esta pregunta es introducirlo en la aplicación de una regresión bayesiana. Con esto, buscaremos entender como calcular una regresión bayesiana en base al "motor" aproximación de Laplace, revisando como se comporta la credibilidad de sus predicciones y como la regresión lineal puede llegar a mutar a aplicar una transformación en el vector $x$. Para responder esta pregunta centre su desarrollo solo en las clases y las funciones que nos brinda la librería `rethinking`.

Unos expertos en alometría deciden realizar un estudio de las alturas de unos niños en un colegio, buscando generar un regresor lineal bayesiano capaz de predecir la altura en base al peso de los alumnos. Para realizar este trabajo recopilan los datos `table_height.csv`, quien posee observaciones fisiológicas de 192 alumnos.

**Parte I**

En conocimiento los datos recopilados por los expertos, le solicitan realizar la siguiente serie de tareas:

- [ ] Defina un modelo de regresión bayesiana, definiendo sus propios priors. Comente cada una de sus asunciones y señale a través de ecuaciones el modelo. Para definir los priors puede ser interesante la información recopilada en el siguiente link: [Priors](https://stacks.cdc.gov/view/cdc/100478)
- [ ] Ajuste el modelo lineal utilizando el método de Laplace approximation. Estudie a través del comando `precis` los resultados obtenidos y coméntelos.
- [ ] Gráfique el MAP de regresión lineal obtenida, añadiendo al gráfico el intervalo del $95\%$ que se tiene sobre la media y los valores predecidos de la altura. Comente los resultados obtenidos y señale si su modelo logra ser un buen predictor de los valores estudiados.


**Parte II**

En base a los resultados obtenidos, el experto que trabaja con usted le señala que las alturas se suelen modelas con pesos logarítmicos, por lo que le sugiere añadir un logaritmo natural en el vector $x$ que compone su modelo lineal. Realice nuevamente la regresión utilizando un intervalo del $95\%$ sobre la media y los valores predecidos de la altura. Comente los resultados obtenidos, señalando si el modelo logra ajustar mejor los valores.

**Respuesta:**

**Parte I**

* **Defina un modelo de regresión bayesiana**

```{r, eval=TRUE}
d <- read.csv("table_height.csv", header = TRUE)
d.hw <- d[ , c("height","weight")]
summary(d.hw)
```

A continuación se define el modelo. La likelihood se asume como una gaussiana. El promedio de alturas es obtenido en base a el peso de la persona ponderado y sumado por los interceptos. Ambos interceptos tienen un prior gaussiano. Por un lado $\beta_{0}$ debiese ser similar al promedio de las alturas, para esto se utiliza un promedio aproximado de las alturas de niños en el estudio revisado. Dado que existen un rango grande de alturas se elige una desviación de 10 en el promedio. 

Por otro lado, $\beta_{1}$ corresponde a la relación de la altura con el peso, tomando como base $\beta_{0}$. Para este valor se escoge una normal, ya que esta abarca las relaciones más probables que estas pueden tener. 

Por último, Se escoge una desviación entre 0 y 40 centimetros, esto dado la alta variedad de alturas en estos rangos de edades.


```{r, eval=TRUE}
model1<-alist( 
    height ~ dnorm( mu, sigma ),
    mu <- b0 + b1*weight,
    b0 ~ dnorm( 125 , 10 ) ,
    b1 ~ dnorm( 0 , 1) ,
    sigma ~ dunif( 0 , 40 )
)
```

* **Ajuste el modelo lineal utilizando el método de Laplace approximation**

Se obtiene un valor $\beta_{0}$, tal como se esperaba, obtiene un valor cercano al promedio de los datos. Por su parte, $\beta_{1}$ obtiene un valor que muestra la relación entre el peso y la altura, que tal como se esperaba es positiva. Por último, $\sigma$ tiene un valor amplio, esto tiene sentido debido a que este estudio se realiza en edades de crecimiento. 



```{r, eval=TRUE}
b.reg1 <- quap(model1, data=d.hw)
precis(b.reg1, prob=0.95)
```

* **Gráfique el MAP de regresión lineal obtenida**

A continuación se muestra la regreción lineal obtenida. Los resultados no logran predecir correctamente los valores estudiados. Aun así, dada la linealización, es posible apreciar que se minimiza el error en relación con los datos.


```{r, eval=TRUE}
# samples from the posterior
post1 <- extract.samples( b.reg1, n= 1e4 )

weight.seq1 <- seq( from=0 , to=50 , by=1 )
mu.link1 <- function(weight) post1$b0 + post1$b1*weight
mu1 <- sapply( weight.seq1 , mu.link1 )
mu.mean1 <- apply( mu1 , 2 , mean )
mu.HPDI1 <- apply( mu1 , 2 , HPDI , prob=0.95 )

height.weight1 <- function(weight)
rnorm(
n=nrow(post1) ,
mean=post1$b0 + post1$b1*weight ,
sd=post1$sigma )

sim.height1 <- sapply( weight.seq1 , height.weight1)
height.HPDI1 <- apply( sim.height1 , 2 , HPDI , prob=0.95 )

plot( height ~ weight , data=d.hw , col=col.alpha(rangi2,0.5) )
lines( weight.seq1 , mu.mean1 )
shade( mu.HPDI1 , weight.seq1 )
shade( height.HPDI1 , weight.seq1 )
```

**Parte II**

A continuación se repite el procedimiento anterior pero esta vez utiliando pesos logarítmicos. Con este cambio se puede apreciar como la regresión se ajusta adecuadamente a los datos, tanto en forma como en la minimización del error.  

```{r, eval=TRUE}
model2<-alist( 
    height ~ dnorm( mu, sigma ),
    mu <- b0 + b1*log(weight),
    b0 ~ dnorm( 125 , 10 ) ,
    b1 ~ dnorm( 0 , 10) ,
    sigma ~ dunif( 0 , 40 )
)
b.reg2 <- quap(model2, data=d.hw)
precis(b.reg2, prob=0.95)
```

```{r, eval=TRUE}
# samples from the posterior
post2 <- extract.samples( b.reg2, n= 1e4 )

weight.seq2 <- seq( from=0 , to=50 , by=1 )
mu.link2 <- function(weight) post2$b0 + post2$b1*log(weight)
mu2 <- sapply( weight.seq2 , mu.link2 )
mu.mean2 <- apply( mu2 , 2 , mean )
mu.HPDI2 <- apply( mu2 , 2 , HPDI , prob=0.95 )

height.weight2 <- function(weight)
rnorm(
n=nrow(post2) ,
mean=post2$b0 + post2$b1*log(weight) ,
sd=post2$sigma )

sim.height2 <- sapply( weight.seq2 , height.weight2)
height.HPDI2 <- apply( sim.height2 , 2 , HPDI , prob=0.95 )

# plot raw data
plot( height ~ weight , data=d.hw , col=col.alpha(rangi2,0.5) )
# draw MAP line
lines( weight.seq2 , mu.mean2 )
# draw HPDI region for line
shade( mu.HPDI2 , weight.seq2 )
# draw HPDI region for simulated heights
shade( height.HPDI2 , weight.seq2 )
```

#### **Pregunta 2:** MCMC

El objetivo de esta pregunta es lograr samplear, mediante la técnica de MCMC, la distribución gamma. 

En general la distribución gamma se denota por $\Gamma(\alpha,\beta)$ donde $\alpha$ y $\beta$ son parámetros positivos, a $\alpha$ se le suele llamar "shape" y a $\beta$ rate La densidad no normalizada de una distribución gamma esta dada por:

$$
f(x\mid \alpha,\beta) = 
\begin{cases}
 x^{\alpha -1}e^{-\beta x} ~ &\text{ si } x > 0\\
0 ~&\text{si } x \leq 0
\end{cases}
$$

- [ ] Implemente el algoritmo de metropolis hasting utilizando $q(\theta^{(t)} \mid \theta^{(t-1)}) = \mathcal{N}(\theta^{t-1},1)$, **importante** su función tiene que poder recibir:
  - [ ] La condición inicial $\theta_{0}$.
  - [ ] Cantidad de repeticiones.
  - [ ] $\alpha$ y $\beta$.
  
  Escriba el algoritmo sin utilizar implementenaciones de la distribución gamma en r. 
  
De ahora en adelante considere $\alpha = 5$ y $\beta = \frac{1}{5}$.

  - [ ] Considere $\theta_{0} = 1$, grafique el histograma que obtiene para distintas cantidad de repeticiones, entre sus pruebas tiene que tener al menos una que tenga del orden de $10^5$ repeticiones ¿Como cambia la distribución en funcion de las repeticiones?
  - [ ] Considere distintos valores de $\theta_{0}$ y una cantidad de repeticiones grande (del orden de $10^5$), grafique las distribuciones que obtenga comente sus resultados  ¿es importante la condición inicial del algoritmo?.
  - [ ] Utilizando $\theta_{0}$ y cantidad de repeticiones conveniente (de acuerdo a lo que obtuvo en las partes anteriores) compare con la distribución real. **Obs:** En esta parte puede utilizar la distribución gamma que tiene implementado r para comparar con lo que usted realizo.

**Respuesta:**


```{r, eval=FALSE}

gamma_fun <- function(x, alpha, beta){
  
  if (x > 0){
    (x ^ (alpha - 1)) * exp(-1*beta * x)
    
  }
  else{
    0
  }
}


MCMC_g_fun <- function(repetitions, omega_i, alpha, beta){
  
  p_omega_t_minus_1 <- omega_i
  #p_omega_a <- omega_i
  historia <- c()
  for(t in 1:repetitions){
    
    p_omega_a <- rnorm(1, mean=p_omega_t_minus_1, sd=1)


    # Suppose we have r
    gf_a <- gamma_fun(p_omega_a,         alpha, beta) / dnorm(x= p_omega_a, mean=p_omega_t_minus_1)
    gf_b <- gamma_fun(p_omega_t_minus_1, alpha, beta) / dnorm(x=p_omega_t_minus_1, mean=p_omega_a)
    
    r_gf <- gf_a / gf_b #gf_a / gf_b
    
    r  <- r_gf
    #print("r")
    #print(r)
    if (rbinom(1, 1, min(1, r)) == 1){
      p_omega_t <- p_omega_a
    }
    else{
      p_omega_t <- p_omega_t_minus_1
    }
    if (p_omega_t <= 0 ){
      p_omega_t = p_omega_t_minus_1
    }
    
    p_omega_a <- p_omega_t
    p_omega_t_minus_1 <- p_omega_a
    historia <- append(historia, p_omega_a)
    
  }
  
  historia
}


```


# P2 a)

A continuación se presentan los resultados e histogramas que se obtienen cuando se modifica
la cantidad de iteraciones del algoritmo.

```{r, eval=TRUE}
library("ggplot2")

history <- MCMC_g_fun(1e2, 1, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()

history <- MCMC_g_fun(1e3, 1, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()

history <- MCMC_g_fun(1e4, 1, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()

history <- MCMC_g_fun(1e5, 1, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()
```

> Podemos observar que a medida que aumenta el número de iteraciones, el histograma
muestra un comportamiento más definido con respecto a la función objetivo.
Y además inconsistencias con la función objetivo, como montes o valles, van desapareciendo cuando las 
repeticiones aumentan.


# P2.b

A continuación se muestran los resultados para distintos valores de $\theta_0$, con el resto de parámetros
fijo.

```{r, eval=TRUE}

history <- MCMC_g_fun(1e5, 1, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()

history <- MCMC_g_fun(1e5, 0.1, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()

history <- MCMC_g_fun(1e5, 0.3, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()

history <- MCMC_g_fun(1e5, 10, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()

```


> Los resultados muestran que la condición inicial  tiene un efecto pequeño cuando
existe una cantidad de iteraciones cercanas a lo 10 miles. El comportamiento o curva del
histograma es el esperado, pero podemos observar algunas diferencias en pequeños montes o valles en la curva, o las diferencias entre el valor máximo de las curvas.


# P2.c

A continuación se comparará la distribución de la función objetivo, versus aquella calculada
usando Metropolis-Hasting

```{r, eval=TRUE}

x <- seq(0, 50, 0.1)
y <- sapply(x, gamma_fun, alpha=5, beta=1/5)
mydf <- data.frame(x = x, res = y)

plot(x, y)



history <- MCMC_g_fun(1e5, 1, 5, 1/5)
history <- data.frame("h_data" = history)
ggplot(history, aes(x=h_data)) + 
  geom_density()



```


> Observando ambas distribuciones, podemos observar que ambas tienen un comportamiento o forma similar, siendo claramente la distribución objetivo más suave y predecible. Para la distribución de Metropolis-Hasting existe cierta variabilidad en algunos rangos que muestran un comportamiento
principalmente aleatorio, esto se cree, debido al carácter de elegibilidad binario que tiene
el algoritmo.

&nbsp;
<hr />
<p style="text-align: center;">A work by <a href="https://github.com/dccuchile/CC6104">CC6104</a></p>

<!-- Add icon library -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">

<!-- Add font awesome icons -->
<p style="text-align: center;">
    <a href="https://github.com/dccuchile/CC6104"><i class="fab fa-github" style='font-size:30px'></i></a>
</p>

<p style="text-align: center;">
    <a href="https://discord.gg/XCbQvGs3Uf"><i class="fab fa-discord" style='font-size:30px'></i></a>
</p>

&nbsp;